{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74a7552",
   "metadata": {},
   "source": [
    "## Score similarity of movie descriptions\n",
    "- given 2 movie descriptions, return a number between 0 and 1\n",
    "- we'll use BERT transformers to get dense sentence(paragraph) embeddings, and then apply a similairty metric\n",
    "- https://github.com/jamescalam/transformers/tree/main/course/similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bc11933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1fd2742",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/stsb-distilbert-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "988ef12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hercules must go from zero to hero to save the universe from Hades.\"\n",
    "\n",
    "# https://huggingface.co/transformers/internal/tokenization_utils.html?highlight=encode_plus#transformers.tokenization_utils_base.PreTrainedTokenizerBase.encode_plus\n",
    "# Tokenize and prepare for the model a sequence or a pair of sequences.\n",
    "tokens = tokenizer.encode_plus(text, max_length=128,\n",
    "                               truncation=True, padding='max_length',\n",
    "                               return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6034b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6157614a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 15067,  2442,  2175,  2013,  5717,  2000,  5394,  2000,  3828,\n",
       "          1996,  5304,  2013, 23003,  1012,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.data['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f6e001b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.1681,  0.0845,  0.3183,  ..., -0.4482,  0.8099, -0.6819],\n",
       "         [ 0.4318, -0.0383,  1.0073,  ..., -0.2067,  0.2495, -0.9275],\n",
       "         [ 0.3545, -0.0485,  0.2166,  ...,  0.1237,  0.0781, -0.6478],\n",
       "         ...,\n",
       "         [-0.1806, -0.2921,  0.5588,  ..., -0.1152,  0.1929, -1.2145],\n",
       "         [-0.1254, -0.2497,  0.7482,  ..., -0.0880,  0.5619, -0.9426],\n",
       "         [-0.2178, -0.2813,  0.6314,  ..., -0.1467,  0.2723, -1.0243]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We process these tokens through our model:\n",
    "outputs = model(**tokens)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d9e2059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1681,  0.0845,  0.3183,  ..., -0.4482,  0.8099, -0.6819],\n",
       "         [ 0.4318, -0.0383,  1.0073,  ..., -0.2067,  0.2495, -0.9275],\n",
       "         [ 0.3545, -0.0485,  0.2166,  ...,  0.1237,  0.0781, -0.6478],\n",
       "         ...,\n",
       "         [-0.1806, -0.2921,  0.5588,  ..., -0.1152,  0.1929, -1.2145],\n",
       "         [-0.1254, -0.2497,  0.7482,  ..., -0.0880,  0.5619, -0.9426],\n",
       "         [-0.2178, -0.2813,  0.6314,  ..., -0.1467,  0.2723, -1.0243]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# produce dense vectors embeddings\n",
    "embeddings = outputs.last_hidden_state\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6487fa96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96ec060d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean pooling to create a single vector encoding\n",
    "# multiply each value in our embeddings tensor by its respective attention_mask value so that we ignore non-real tokens\n",
    "\n",
    "# resize attention_mask tensor\n",
    "attention_mask = tokens['attention_mask']\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abc3c9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b561e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b75d5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3df5cd39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc906352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_embeddings = embeddings * mask\n",
    "masked_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f93e79da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1681,  0.0845,  0.3183,  ..., -0.4482,  0.8099, -0.6819],\n",
       "         [ 0.4318, -0.0383,  1.0073,  ..., -0.2067,  0.2495, -0.9275],\n",
       "         [ 0.3545, -0.0485,  0.2166,  ...,  0.1237,  0.0781, -0.6478],\n",
       "         ...,\n",
       "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93993730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed = torch.sum(masked_embeddings, 1)\n",
    "summed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b3b1992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "summed_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b544b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "         16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48c1467",
   "metadata": {},
   "source": [
    "Finally, we calculate the mean as the sum of the embedding activations `summed` divided by the number of values that should be given attention in each position `summed_mask`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d539a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pooled = summed / summed_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7789ed81",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.7997e-02,  3.9717e-02,  3.1979e-01, -1.4136e-01,  5.5878e-01,\n",
       "          8.2253e-02,  5.0833e-01,  6.5488e-01,  3.4281e-01, -6.8770e-01,\n",
       "         -4.9388e-01,  6.8750e-01, -1.1760e+00, -2.5420e-01,  2.1274e-01,\n",
       "         -5.6078e-01, -6.0346e-01, -2.0626e-01, -9.3915e-02, -2.7426e-01,\n",
       "          8.3787e-02,  1.7910e-01,  1.8278e-01,  4.8750e-01, -8.0592e-01,\n",
       "          1.0510e+00, -9.6357e-02, -1.5066e-01,  1.2120e-01,  3.8352e-01,\n",
       "          5.1736e-01,  5.0449e-01, -5.2063e-01, -2.3304e-01, -3.3285e-01,\n",
       "          3.4771e-01,  2.4014e-01, -4.6893e-01,  9.0721e-01, -2.3205e-01,\n",
       "          4.2056e-01,  2.9983e-01,  3.3246e-01, -4.8929e-01, -1.0834e+00,\n",
       "         -7.1698e-01, -1.9738e-01, -6.5017e-01,  4.4382e-02, -5.4016e-01,\n",
       "         -2.4586e-01, -2.3958e-01, -5.5259e-02,  4.7614e-01,  8.4970e-01,\n",
       "          5.8249e-02, -1.1031e+00, -2.0930e-02, -1.3984e-01,  7.5236e-01,\n",
       "         -3.4633e-01,  4.5343e-01, -4.1268e-01, -2.2891e-01, -4.4620e-01,\n",
       "         -6.1007e-01, -1.4399e-01,  3.9702e-02,  2.6007e-01, -1.0209e+00,\n",
       "         -1.7637e-01, -2.1375e-01, -3.5435e-01,  1.1952e-01,  6.5598e-02,\n",
       "          1.1956e-02, -8.8092e-01, -8.0271e-02,  2.1005e-01, -1.5341e-01,\n",
       "         -6.3597e-01, -2.4111e-01,  3.0773e-01,  1.0026e+00, -1.6890e-01,\n",
       "          1.8423e-01, -1.5288e-01,  2.7891e-01,  2.0873e-01,  3.9766e-01,\n",
       "         -7.8237e-02, -6.0651e-01,  6.5169e-01, -2.7303e-01, -3.2407e-01,\n",
       "         -3.2503e-01,  6.9616e-01, -2.5969e-01, -3.2444e-01, -6.3624e-02,\n",
       "         -9.4948e-01, -4.4196e-02, -1.0061e-01, -9.1937e-01, -4.5576e-01,\n",
       "         -3.5238e-01, -2.7668e-02,  1.3501e+00,  2.4207e-01,  5.5930e-02,\n",
       "         -3.4982e-01, -1.0072e-01,  1.8534e-01,  3.3410e-01,  7.4465e-01,\n",
       "         -4.1933e-01, -6.6941e-01,  4.3069e-01,  7.1965e-01,  1.9985e-01,\n",
       "          4.1015e-01,  4.5337e-02, -6.2536e-02,  7.4374e-01,  1.2351e-01,\n",
       "         -6.2895e-01,  3.7060e-01, -3.9733e-01,  7.8320e-01,  1.0385e+00,\n",
       "         -4.1456e-02,  2.8402e-01, -1.0717e+00,  1.5899e-01, -3.9525e-02,\n",
       "          6.2751e-01, -2.8118e-01,  4.2814e-01,  3.8652e-02, -1.1815e+00,\n",
       "          1.1878e-01,  5.9365e-02, -3.8576e-01,  4.2542e-01,  2.8469e-02,\n",
       "          1.1783e+00,  6.9691e-01,  1.5588e-01,  2.9526e-01, -1.6265e-01,\n",
       "         -8.9026e-02, -7.4055e-01, -5.2990e-01,  3.7529e-01,  5.3724e-01,\n",
       "         -1.8910e-01,  9.7115e-01,  5.1602e-01, -7.9939e-01,  3.2677e-01,\n",
       "          1.6944e-01, -4.2509e-01,  4.2001e-01,  4.4356e-01, -3.2857e-02,\n",
       "          4.6705e-01,  4.2802e-01, -4.4453e-01, -7.5718e-01, -2.9459e-01,\n",
       "          4.4748e-01, -8.8791e-02, -1.3575e-01,  1.0496e+00, -9.6283e-01,\n",
       "          1.6228e-01,  3.7742e-01,  2.8950e-01, -3.9997e-01, -6.7182e-01,\n",
       "         -6.6089e-01,  3.5966e-01, -4.4997e-01, -3.2506e-01,  2.9199e-01,\n",
       "         -5.0403e-01,  7.0310e-01, -2.6948e-01,  5.0662e-01,  7.4172e-01,\n",
       "         -1.0993e+00, -6.2169e-02,  7.6861e-01,  3.5926e-01,  5.5220e-01,\n",
       "         -1.0927e-01, -2.0950e-01, -1.0049e+00,  5.6732e-01,  4.2273e-01,\n",
       "          4.6510e-01, -1.5549e+00,  7.7424e-01, -7.9265e-01,  1.2001e+00,\n",
       "         -2.0815e-01, -1.2825e-01,  9.7388e-02,  1.8432e-01, -1.4594e-01,\n",
       "          7.9685e-01, -4.9865e-01, -7.1980e-01, -2.8851e-01,  4.7246e-01,\n",
       "         -9.3278e-01,  5.5297e-01, -7.4218e-01, -3.9603e-01, -3.7297e-01,\n",
       "          7.2440e-01, -4.1134e-01,  4.4896e-01, -1.5985e+00,  8.6505e-02,\n",
       "          2.5379e-01,  2.6208e-01,  1.1947e-02,  7.0500e-01, -3.7122e-02,\n",
       "         -1.3156e+00, -1.3963e-01, -6.1171e-01,  1.3803e+00,  5.3406e-01,\n",
       "          2.4942e-01,  1.4863e-01, -8.8493e-01, -8.1685e-01, -6.4527e-01,\n",
       "         -2.4197e-01, -1.1741e+00, -1.3331e-01, -2.5084e-01,  4.4957e-01,\n",
       "          6.2226e-01, -6.2211e-01, -1.2126e+00, -9.9267e-02,  3.5093e-02,\n",
       "         -6.4157e-01, -1.2618e+00, -5.6198e-02,  5.4659e-01,  1.2594e+00,\n",
       "         -4.7827e-01, -6.2665e-02,  6.2855e-01, -2.7615e-01, -6.7704e-01,\n",
       "          1.5080e-01, -7.1243e-01,  3.5691e-01, -1.6100e-01,  2.2935e-01,\n",
       "          2.3208e-02, -3.7435e-01,  4.7992e-01, -4.8346e-01, -4.1325e-01,\n",
       "          5.4230e-01, -6.0433e-01,  2.8112e-01,  7.9960e-01,  1.2956e+00,\n",
       "          7.5875e-01, -3.1558e-02, -4.8946e-01,  3.0011e-01,  2.3467e-01,\n",
       "          4.3345e-01,  5.2963e-01,  1.4517e-01, -3.6432e-01,  3.6031e-01,\n",
       "          1.7168e-01,  6.3442e-01, -7.4816e-02, -7.7798e-01,  7.4891e-02,\n",
       "          5.9177e-01,  1.7419e-01,  9.4441e-02,  4.9720e-02,  5.4822e-01,\n",
       "          5.5604e-01,  4.1056e-01,  5.8111e-01, -4.3728e-01,  3.2914e-01,\n",
       "         -2.3211e-01, -3.4139e-01, -2.5882e-01,  2.6966e-01, -9.9029e-02,\n",
       "          2.5134e-01, -8.2061e-02,  5.2028e-01,  6.1765e-02,  2.6486e-01,\n",
       "          1.7374e-02, -9.4768e-02, -2.3502e-01,  2.2992e-01,  5.9744e-01,\n",
       "          5.3236e-01,  4.3940e-01,  1.4684e-01, -8.5720e-02, -1.6272e-01,\n",
       "          3.7172e-01,  3.4902e-01, -7.3212e-01, -8.4635e-02, -2.3259e-01,\n",
       "          1.5050e+00, -3.8051e-01,  2.3604e-01,  4.9112e-01, -6.1487e-01,\n",
       "         -7.0016e-01, -5.7129e-01, -2.3038e-01, -4.2600e-01,  2.8699e-01,\n",
       "          3.1516e-02,  3.5870e-01,  4.4564e-01, -1.2499e-01, -8.7196e-01,\n",
       "         -4.2445e-02, -4.1497e-01,  8.3719e-02,  1.6832e-01, -1.9913e-01,\n",
       "          3.8262e-01,  8.2147e-02, -3.9011e-01, -3.8416e-01, -4.2049e-01,\n",
       "         -4.7174e-01, -2.4274e-01,  1.5934e+00, -1.7583e-01,  1.1141e+00,\n",
       "          4.9052e-01,  7.6858e-02,  7.6569e-01,  1.4176e+00, -3.4866e-02,\n",
       "         -3.3760e-01, -6.8012e-01,  2.4362e-01,  1.9663e-01,  2.2584e-01,\n",
       "          3.8287e-01,  3.2358e-01, -5.9812e-01,  4.1713e-02,  1.1512e-01,\n",
       "          6.0317e-02, -2.1651e-01, -7.5443e-01,  7.5293e-01, -7.5467e-01,\n",
       "          5.1268e-01, -7.5195e-01,  6.0019e-01, -6.3893e-01, -4.8785e-02,\n",
       "         -1.2200e+00, -5.7337e-01, -8.7459e-01,  3.6971e-01, -3.8029e-01,\n",
       "         -4.8693e-01, -1.6600e-01,  1.6630e-03, -4.5224e-01, -7.0285e-02,\n",
       "          6.8694e-02, -5.4130e-01,  7.7324e-01,  5.0658e-01, -5.5058e-01,\n",
       "          3.2045e-01, -1.4657e-01, -1.1697e+00, -2.4790e-01, -2.8122e-01,\n",
       "         -5.2562e-01,  7.8291e-01,  2.2196e-01, -4.7039e-01, -2.0356e-01,\n",
       "          3.1980e-01, -4.8002e-01,  7.1744e-01, -6.1864e-01, -1.3801e-01,\n",
       "         -1.0419e-01, -2.2063e-02, -9.4135e-01,  8.0146e-01, -3.5115e-01,\n",
       "         -3.6946e-01,  4.7270e-02, -7.3010e-01,  1.2883e-01,  5.0497e-01,\n",
       "          4.6018e-01, -6.3152e-01, -2.8007e-01,  4.8751e-01,  4.6724e-01,\n",
       "          8.1057e-01,  3.2316e-01, -1.3989e-01, -2.3974e-01,  1.7216e-01,\n",
       "          7.7178e-01, -7.6374e-01, -9.3685e-01,  6.4222e-01, -7.0832e-01,\n",
       "          4.8142e-01,  9.3868e-01, -6.1582e-01,  2.9993e-01,  4.9004e-01,\n",
       "          7.4783e-01,  1.6456e+00,  2.6898e-01, -2.7521e-01,  2.6896e-01,\n",
       "          8.3632e-01, -2.3172e-01,  2.6624e-01,  1.7118e-01,  4.2688e-01,\n",
       "         -5.9934e-01, -6.4007e-01,  2.4909e-01, -5.4604e-01, -7.2803e-02,\n",
       "         -7.1545e-02, -2.1092e-01, -8.9475e-01,  2.9058e-02,  5.8642e-03,\n",
       "         -7.0403e-01, -1.4251e+00, -9.7635e-02, -2.7743e-01, -8.2699e-01,\n",
       "         -4.0544e-01,  6.2059e-01, -1.4355e+00,  8.0472e-01,  2.5523e-02,\n",
       "         -9.1326e-02,  8.4809e-01,  3.3542e-01, -4.7797e-01, -1.9582e-01,\n",
       "         -1.0470e+00, -7.0837e-01, -9.9208e-01, -7.1545e-01,  3.8491e-01,\n",
       "          5.1971e-02,  3.4104e-01, -3.7771e-01, -4.0668e-01, -3.1584e-01,\n",
       "         -6.1694e-01,  4.6531e-01,  2.1272e-02,  7.5740e-01, -1.5889e-01,\n",
       "         -1.0317e+00, -1.3803e+00, -3.7681e-02, -1.9865e-01,  5.3407e-01,\n",
       "         -2.7750e-01,  8.8244e-01, -1.9362e-03,  3.4952e-01,  3.2988e-01,\n",
       "         -5.9265e-01, -4.9650e-02, -8.1664e-02, -3.4795e-02, -1.1398e-03,\n",
       "          3.1566e-01, -9.0405e-02,  1.7138e+00, -6.6406e-01,  7.4454e-02,\n",
       "         -5.0440e-01, -3.4521e-01, -8.3489e-02, -7.7307e-02, -7.7160e-01,\n",
       "          6.4362e-01,  3.1640e-02,  7.5792e-01, -5.2230e-01,  5.5782e-01,\n",
       "         -5.4264e-01, -1.3034e-01, -3.7908e-01,  7.1039e-02,  6.3238e-01,\n",
       "         -3.0574e-01,  2.5846e-01, -2.8894e-01, -1.3902e-01, -6.3784e-03,\n",
       "         -2.8409e-02,  2.6589e-01,  7.7651e-01, -6.3083e-01,  9.8920e-01,\n",
       "         -1.7910e-01, -1.1890e-01, -9.5667e-01,  3.0034e-01, -1.5251e-01,\n",
       "         -1.7607e-01,  1.9224e-01,  7.6527e-01,  7.7858e-02,  1.0487e+00,\n",
       "         -8.9411e-01, -4.8993e-01,  1.8891e-01, -5.5077e-01, -1.0414e-01,\n",
       "         -5.8109e-01, -7.9329e-01,  1.1104e+00, -6.5754e-02,  8.2173e-01,\n",
       "          3.6458e-01, -1.5242e-01, -7.8763e-01,  7.9172e-01,  4.9191e-01,\n",
       "         -1.2464e+00,  3.7802e-01,  3.9952e-01, -1.3190e+00, -2.2491e-01,\n",
       "          1.6039e-01, -8.0600e-03, -2.4648e-01, -2.4613e-01,  8.4780e-02,\n",
       "          8.7638e-02, -1.2524e-01,  6.5409e-01,  8.1841e-02, -2.7493e-01,\n",
       "          1.1454e-01,  5.1877e-02, -3.2620e-01,  6.8889e-01, -4.2459e-01,\n",
       "         -1.1345e-01, -6.8246e-01, -9.4187e-02,  1.7741e-01,  1.2625e-01,\n",
       "         -5.3160e-02,  2.0932e-01,  7.5159e-02, -1.0698e+00, -4.5414e-02,\n",
       "         -2.2749e-01,  2.6001e-01,  1.3880e-02,  5.2504e-02,  8.9358e-01,\n",
       "         -6.9896e-01,  2.2999e-02,  1.5356e-01,  4.2496e-01, -2.3091e-01,\n",
       "          5.0243e-01, -1.6904e+00, -1.4791e-01,  3.6546e-01, -9.0046e-01,\n",
       "         -8.9529e-02, -7.3796e-02,  2.5090e-01, -4.6017e-01, -6.9293e-02,\n",
       "         -4.3681e-01,  3.0375e-01, -1.6833e-01,  5.1218e-02,  3.4103e-01,\n",
       "         -8.7835e-01, -2.5700e-01, -5.2261e-01, -2.6458e-01,  4.4900e-01,\n",
       "         -6.5239e-01, -3.4979e-01, -5.0085e-01, -7.7384e-01,  5.3296e-01,\n",
       "         -2.7100e-01, -3.1249e-01,  6.5356e-01,  2.6054e-01, -3.0312e-01,\n",
       "          2.6560e-01,  3.2123e-01, -4.3319e-01,  2.2834e-01, -1.3917e-01,\n",
       "          5.8079e-01,  5.8455e-01,  1.2836e-01,  9.6190e-01, -7.4467e-01,\n",
       "          1.3537e-01, -5.3154e-01, -4.6156e-01, -5.1643e-01,  5.4422e-01,\n",
       "         -8.1254e-01, -5.3542e-01, -7.6655e-02,  3.4871e-01, -4.3962e-01,\n",
       "         -1.8171e-03,  3.0398e-01, -1.7542e-01,  4.3383e-01, -5.4385e-01,\n",
       "          4.4535e-01,  1.3620e-01, -1.4431e-01,  2.4187e-01,  2.9035e-01,\n",
       "         -1.4682e+00, -6.6929e-01,  1.6904e-01,  9.2061e-01,  8.3737e-01,\n",
       "         -3.2701e-01, -1.4683e-02,  4.6505e-01,  9.6637e-01, -2.0077e-01,\n",
       "          6.7213e-01, -5.2917e-01,  1.9613e-02,  3.1233e-01, -3.9483e-01,\n",
       "         -8.3507e-01,  5.3177e-02,  9.6257e-01,  9.2589e-02, -9.3182e-02,\n",
       "         -1.0078e+00, -2.2117e-01,  4.5826e-01, -2.2293e-01, -4.8894e-01,\n",
       "          8.3084e-02,  6.1426e-01,  6.3188e-02,  5.8852e-01, -7.0125e-01,\n",
       "          5.1650e-02,  1.1044e+00,  6.7790e-02, -2.6669e-01,  7.1993e-01,\n",
       "         -5.8521e-01,  4.4480e-01,  3.6117e-01, -7.3460e-01,  8.9260e-01,\n",
       "         -3.3682e-01, -2.7565e-01,  8.8490e-01, -5.4447e-01, -5.7995e-01,\n",
       "         -1.0496e-01, -5.6990e-01, -4.3767e-02,  2.9523e-01,  2.1535e-01,\n",
       "         -1.4276e-01, -1.4029e+00,  9.1226e-01, -1.1613e-01,  3.2579e-01,\n",
       "         -1.8325e-01, -5.1146e-01,  8.4618e-01,  7.8720e-01,  5.9088e-01,\n",
       "          1.8436e-01, -4.4053e-01,  3.3113e-01, -2.8215e-01, -6.8031e-01,\n",
       "         -3.7158e-01, -3.1354e-01,  5.9482e-01, -8.8503e-01, -1.8802e-01,\n",
       "         -3.4828e-02,  1.6450e-01, -3.4967e-01,  4.5469e-01, -1.3027e-01,\n",
       "         -4.3035e-02,  3.3624e-01,  7.4993e-02,  1.2137e+00, -5.9948e-02,\n",
       "         -1.0848e+00, -5.1351e-01, -1.1950e+00,  9.2066e-02, -7.0829e-02,\n",
       "          1.9402e-01, -6.5817e-01, -4.1158e-01, -6.6547e-01,  6.1754e-01,\n",
       "         -8.1140e-01,  3.0612e-01,  9.0194e-01,  4.7759e-04, -5.7801e-01,\n",
       "         -1.9997e-01, -2.9811e-01, -1.5565e-02, -1.5391e-01,  2.0065e-01,\n",
       "          1.2876e+00, -3.3874e-01,  6.2683e-01,  1.0484e+00,  6.1508e-02,\n",
       "         -1.9593e-01,  2.8455e-01, -8.3252e-01]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e1a5791",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "545ff2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h\n",
    "v = h.calculate_sentence_embedding(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35463d62",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26482e73",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-10d17282b9ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# calculate\n",
    "cosine_similarity(\n",
    "    [mean_pooled[0]],\n",
    "    mean_pooled[1:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4f49e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
